# -*- coding: utf-8 -*-
"""Twitter_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FPuOQiuVv3pj3BFiigcG7SiqqSxD1_Jf
"""

import pandas as pd 
import numpy as np
import seaborn as sns 
import matplotlib.pyplot as plt 
from jupyterthemes import jtplot
jtplot.style(theme = 'monokai', context = 'notebook', ticks = True , grid= False)

pip install jupyterthemes

import os
for dirname, _, filenames in os.walk(r"C:\Users\Sourav dhaniya\Desktop\keggle\Twitter Sentimental alaysis"):
    for filename in filenames:
        print(os.path.join(dirname, filename))

tweets_test_df = pd.read_csv('/test.csv')
tweets_train_df = pd.read_csv('/train.csv')

tweets_train_df.info()

tweets_train_df.describe()

tweets_train_df['tweet']

tweets_train_df = tweets_train_df.drop(['id'], axis=1)

tweets_train_df

sns.heatmap(tweets_train_df.isnull(), yticklabels = False, cbar = False, cmap= "Blues")

tweets_train_df.hist(bins = 30, figsize=(15,5), color='r')

sns.countplot(tweets_train_df['label'], label='count')

tweets_train_df['length'] = tweets_train_df['tweet'].apply(len)

tweets_train_df

tweets_train_df['length'].plot(bins=100, kind='hist')

tweets_train_df.describe()

tweets_train_df[tweets_train_df['length'] == 11]['tweet'].iloc[0]

positive = tweets_train_df[tweets_train_df['label'] == 0 ]

negative = tweets_train_df[tweets_train_df['label'] == 1 ]

positive

negative

sentences = tweets_train_df['tweet'].tolist()

sentences

len(sentences)

sentences_as_one_string = " ".join(sentences)

from wordcloud import WordCloud

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))

negative_list = negative['tweet'].tolist()

negative_sentences_as_one_string = " ".join(negative_list)

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(negative_sentences_as_one_string))

"""Perform data cleaning"""

import string 
string.punctuation

Test = 'You are the most beautiful women i  ever meet :) and i want to be with u *_*'

Test_punct_removed = [ char for char in Test if char not in string.punctuation]

Test_punct_removed

Test_punct_removed_join = "".join(Test_punct_removed)

Test_punct_removed_join



"""Remove punct using a different method """

Test_punct_removed = []
for  char in Test:
  if  char not in string.punctuation:
    Test_punct_removed.append(char)

Test_punct_removed_join = "".join(Test_punct_removed)    
Test_punct_removed_join



from nltk.corpus import stopwords
stopwords.words('english')

import nltk
nltk.download("stopwords")

Test_punct_removed_join_clean = [ word for  word in Test_punct_removed_join.split() if word.lower() not  in stopwords.words('english') ]

Test_punct_removed_join_clean



"""Remove  word and punctuation using a pipeline """

mini_challenge = "Here is a mini challenge, that will teach u how to remove punctuations and stopwords*_*"

challenge = [ char for  char in mini_challenge if char not in string.punctuation]
challenge= "".join(challenge)
challenge = [ word for word in challenge.split() if word.lower() not in stopwords.words('english')]

challenge

"""Perform Tokenization"""

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first paper', 'This is the second paper', 'This is the third and last paper']
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sample_data)

print(vectorizer.get_feature_names_out())

print(X.toarray())

def message_cleaning(message):
  Test_punct_removed = [char for char in message if char not in string.punctuation]
  Test_punct_removed_join = ''.join(Test_punct_removed)
  Test_punct_removed_join_clean = [ word for word in Test_punct_removed_join.split() if word.lower() not in stopwords.words('english') ]
  return Test_punct_removed_join_clean

tweets_df_clean = tweets_train_df['tweet'].apply(message_cleaning)

print(tweets_df_clean[5])

print(tweets_train_df['tweet'][5])

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(analyzer = message_cleaning)
tweets_countvectorizer = CountVectorizer(analyzer = message_cleaning, dtype= 'uint8').fit_transform(tweets_train_df['tweet']).toarray()

tweets_countvectorizer.shape

x = tweets_countvectorizer

y = tweets_train_df['label']

x.shape

y.shape

"""Naive bayes"""

from sklearn.naive_bayes import MultinomialNB
NB_classifier = MultinomialNB()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)

from sklearn.naive_bayes import MultinomialNB
NaiveBclassifier = MultinomialNB()
NaiveBclassifier.fit(X_train,y_train)

pip install --upgrade scikit-learn

from sklearn.metrics import classification_report, confusion_matrix

y_pred_test = NaiveBclassifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred_test)
sns.heatmap(cm, annot= True)

print(classification_report(y_test, y_pred_test))

